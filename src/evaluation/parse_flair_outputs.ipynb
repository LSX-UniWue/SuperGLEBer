{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1635427c26b922ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T18:22:07.591236Z",
     "start_time": "2024-08-05T18:22:07.587705Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import yaml\n",
    "from loguru import logger\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef321ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T18:22:07.918344Z",
     "start_time": "2024-08-05T18:22:07.794720Z"
    }
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367f90764ff2a85f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T18:22:08.113674Z",
     "start_time": "2024-08-05T18:22:08.084912Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"model\", \"task\", \"seed\", \"source\", \"micro-f1\", \"macro-f1\", \"acc\", \"Pearson\", \"qa-f1\"])\n",
    "\n",
    "# Iterate over directories\n",
    "for day in Path(\"/Users/juliawunderle/translator/src/outputs/\").iterdir():\n",
    "    for run in day.iterdir():\n",
    "        if day.name == \"slurm_logs\":\n",
    "            continue\n",
    "        logger.info(f\"Processing: {run}\")\n",
    "\n",
    "        config_path = run / \"hydra_conf/config.yaml\"\n",
    "        if config_path.exists():\n",
    "            with open(config_path) as f:\n",
    "                cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "                task_name = cfg[\"task\"][\"task_name\"] if \"task\" in cfg and \"task_name\" in cfg[\"task\"] else None\n",
    "\n",
    "        if task_name is None:\n",
    "            logger.warning(f\"Skipping {run} as it has no task name\")\n",
    "            continue\n",
    "\n",
    "        logger.info(f\"Task name: {task_name}, Model name: {cfg['model']['model_name']}\")\n",
    "\n",
    "        if task_name == \"similarity_pawsx\":\n",
    "            training_log_path = run / task_name / \"training_logs\" / \"logfile.log\"\n",
    "            if not training_log_path.exists():\n",
    "                logger.warning(f\"Skipping {run} as it has no training log\")\n",
    "                continue\n",
    "            with open(training_log_path, \"r\") as log_file:\n",
    "                log_content = log_file.read()\n",
    "                if \"Cosine-Similarity Pearson\" not in log_content:\n",
    "                    logger.warning(f\"Skipping {run} as it did not finish\")\n",
    "                    continue\n",
    "                pearson = re.search(r\"Cosine-Similarity Pearson: (\\d+\\.\\d+)\", log_content)\n",
    "                if pearson:\n",
    "                    pearson = pearson.group(1)\n",
    "                else:\n",
    "                    logger.warning(f\"Skipping {run} as it has no Pearson value\")\n",
    "                    continue\n",
    "\n",
    "                results = {\"micro-f1\": None, \"macro-f1\": None, \"acc\": None, \"Pearson\": pearson, \"qa-f1\": None}\n",
    "        elif task_name == \"mlqa\":\n",
    "            training_log_path = run / task_name / \"training_logs\" / \"train.log\"\n",
    "            if not training_log_path.exists():\n",
    "                logger.warning(f\"Skipping {run} as it has no training log\")\n",
    "                continue\n",
    "            with open(training_log_path, \"r\") as log_file:\n",
    "                log_content = log_file.read()\n",
    "                if \"eval_f1_score\" not in log_content:\n",
    "                    logger.warning(f\"Skipping {run} as it did not finish\")\n",
    "                    continue\n",
    "\n",
    "                # Adjusted regex pattern for `eval_f1_score`\n",
    "                qa_f1_match = re.search(r\"eval_f1_score':\\s*([\\d.]+)\", log_content)\n",
    "                if qa_f1_match:\n",
    "                    qa_f1 = qa_f1_match.group(1)\n",
    "                else:\n",
    "                    logger.warning(f\"Skipping {run} as it has no QA-F1 value\")\n",
    "                    continue\n",
    "\n",
    "                results = {\"micro-f1\": None, \"macro-f1\": None, \"acc\": None, \"Pearson\": None, \"qa-f1\": qa_f1}\n",
    "\n",
    "        # FLair format\n",
    "        else:\n",
    "            training_log_path = run / task_name / \"training_logs\" / \"training.log\"\n",
    "            if not training_log_path.exists():\n",
    "                logger.warning(f\"Skipping {run} as it has no training log\")\n",
    "                continue\n",
    "            with open(training_log_path, \"r\") as log_file:\n",
    "                log_content = log_file.read()\n",
    "                if \"Results:\" not in log_content:\n",
    "                    logger.warning(f\"Skipping {run} as it did not finish\")\n",
    "                    continue\n",
    "\n",
    "                micro_f1 = re.search(r\"F-score \\(micro\\) (\\d+\\.\\d+)\", log_content)\n",
    "                macro_f1 = re.search(r\"F-score \\(macro\\) (\\d+\\.\\d+)\", log_content)\n",
    "                acc = re.search(r\"Accuracy (\\d+\\.\\d+)\", log_content)\n",
    "\n",
    "                if micro_f1 and macro_f1 and acc:\n",
    "                    results = {\n",
    "                        \"micro-f1\": micro_f1.group(1),\n",
    "                        \"macro-f1\": macro_f1.group(1),\n",
    "                        \"acc\": acc.group(1),\n",
    "                        \"Pearson\": None,\n",
    "                        \"qa-f1\": None,\n",
    "                    }\n",
    "                else:\n",
    "                    logger.warning(f\"Skipping {run} as it has incomplete results\")\n",
    "                    continue\n",
    "\n",
    "        df = pd.concat(\n",
    "            [\n",
    "                df,\n",
    "                pd.DataFrame(\n",
    "                    data=[\n",
    "                        [\n",
    "                            cfg[\"model\"][\"model_name\"],\n",
    "                            task_name,\n",
    "                            cfg[\"seed\"],\n",
    "                            f\"{day.name}/{run.name}\",\n",
    "                            results[\"micro-f1\"],\n",
    "                            results[\"macro-f1\"],\n",
    "                            results[\"acc\"],\n",
    "                            results[\"Pearson\"],\n",
    "                            results[\"qa-f1\"],\n",
    "                        ]\n",
    "                    ],\n",
    "                    columns=[\"model\", \"task\", \"seed\", \"source\", \"micro-f1\", \"macro-f1\", \"acc\", \"Pearson\", \"qa-f1\"],\n",
    "                ),\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "df.sort_values(by=[\"task\", \"model\", \"seed\"], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c7f4a01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T18:22:12.392054Z",
     "start_time": "2024-08-05T18:22:12.387105Z"
    }
   },
   "outputs": [],
   "source": [
    "# Metric Mapping\n",
    "metrics = {\n",
    "    \"argument_mining\": \"macro-f1\",\n",
    "    \"db_aspect\": \"micro-f1\",\n",
    "    \"engaging_comments\": \"macro-f1\",\n",
    "    \"factclaiming_comments\": \"macro-f1\",\n",
    "    \"germanquad\": \"qa-f1\",\n",
    "    \"germeval_opinion\": \"micro-f1\",\n",
    "    \"hotel_aspect\": \"micro-f1\",\n",
    "    \"massive_intents\": \"micro-f1\",\n",
    "    \"massive_seq\": \"micro-f1\",\n",
    "    \"mlqa\": \"qa-f1\",\n",
    "    \"ner_biofid\": \"micro-f1\",\n",
    "    \"ner_europarl\": \"micro-f1\",\n",
    "    \"ner_legal\": \"micro-f1\",\n",
    "    \"ner_news\": \"micro-f1\",\n",
    "    \"ner_wiki_news\": \"micro-f1\",\n",
    "    \"nli\": \"acc\",\n",
    "    \"offensive_lang\": \"macro-f1\",\n",
    "    \"pawsx\": \"acc\",\n",
    "    \"polarity\": \"micro-f1\",\n",
    "    \"query_ad\": \"acc\",\n",
    "    \"quest_ans\": \"acc\",\n",
    "    \"similarity_pawsx\": \"Pearson\",\n",
    "    \"topic_relevance\": \"micro-f1\",\n",
    "    \"toxic_comments\": \"macro-f1\",\n",
    "    \"up_dep\": \"micro-f1\",\n",
    "    \"up_pos\": \"micro-f1\",\n",
    "    \"verbal_idioms\": \"micro-f1\",\n",
    "    \"webcage\": \"micro-f1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5bbf8ac6fae095b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T18:22:13.327336Z",
     "start_time": "2024-08-05T18:22:13.320374Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add new results to existing results.json or create new results.json\n",
    "new_records = df.to_dict(orient=\"records\")\n",
    "\n",
    "json_path = Path(\"/Users/juliawunderle/translator/src/evaluation/results_new.json\")\n",
    "if json_path.exists() and json_path.stat().st_size != 0:\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "else:\n",
    "    data = []\n",
    "\n",
    "for new_record in new_records:\n",
    "    model_name = new_record[\"model\"].split(\"/\")[1]\n",
    "    if \"gottbert\" in model_name:\n",
    "        model_name = \"gottbert\"\n",
    "    task_name = new_record[\"task\"]\n",
    "\n",
    "    metric_type = metrics.get(task_name)\n",
    "\n",
    "    if metric_type and new_record[metric_type] is not None:\n",
    "        existing_record = next((item for item in data if item[\"model\"] == model_name), None)\n",
    "\n",
    "        if existing_record:\n",
    "            existing_record[task_name] = new_record[metric_type]\n",
    "        else:\n",
    "            new_entry = {\"model\": model_name, task_name: new_record[metric_type]}\n",
    "            data.append(new_entry)\n",
    "\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
