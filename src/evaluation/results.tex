\begin{table}
\caption{Model performance across tasks}
\label{tab:results}
\begin{tabular}{lllllllll}
\toprule
 & flausch_classification & flausch_tagging & harmful_content_c2a & harmful_content_dbo & harmful_content_vio & llms4subjects & sustaineval_classification & sustaineval_regression \\
model &  &  &  &  &  &  &  &  \\
\midrule
/hnvme/workspace/b185cb13-llammlein/anvme/models/120M_fa3_new/models_hf/iter-01430512-ckpt_120M & 0.924 & 0.4328 & 0.9196 & 0.8449 & 0.943 & 0.6408 & 0.2247 & 0.3299 \\
/hnvme/workspace/b185cb13-llammlein/anvme/models/tiny_LLaMA_1B_fa3_new/models_hf/iter-01430512-ckpt_1B & 0.9414 & 0.5411 & 0.9361 & 0.886 & 0.9446 & 0.6683 & 0.4494 & 0.3207 \\
DiscoResearch/Llama3-German-8B & 0.9487 & 0.5271 & 0.9443 & 0.8709 & 0.9502 & 0.6818 & 0.6255 & 0.4301 \\
EuroBERT/EuroBERT-2.1B & 0.8983 & 0.3886 & 0.8941 & 0.7728 & 0.9309 & 0.6664 & 0.1273 & 0.151 \\
EuroBERT/EuroBERT-210m & 0.9062 & 0.2963 & 0.8877 & 0.7846 & 0.9309 & 0.6209 & 0.1199 & 0.1215 \\
EuroBERT/EuroBERT-610m & 0.9066 & 0.4936 & 0.8776 & 0.8173 & 0.9309 & 0.6569 & 0.1011 & 0.2089 \\
FacebookAI/xlm-roberta-base & 0.9231 & 0.3083 & 0.8977 & 0.8223 & 0.9309 & 0.6073 & 0.1348 & 0.272 \\
GeistBERT/GeistBERT_base & 0.9329 & 0.4807 & 0.9269 & 0.8424 & 0.9349 & 0.6327 & 0.2772 & 0.2284 \\
LSX-UniWue/LLaMmlein_7B & 0.9498 & 0.5753 & 0.9516 & 0.8868 & 0.9558 & 0.6804 & 0.633 & 0.3567 \\
LSX-UniWue/ModernGBERT_134M & 0.9325 & 0.5199 & 0.9315 & 0.865 & 0.9357 & 0.6608 & 0.2097 & 0.243 \\
LSX-UniWue/ModernGBERT_1B & 0.9475 & 0.6619 & 0.9534 & 0.8969 & 0.9518 & 0.6848 & 0.6592 & 0.1182 \\
LeoLM/leo-hessianai-7b & 0.9526 & 0.5208 & 0.9516 & 0.8969 & 0.9486 & 0.6821 & 0.6217 & 0.3488 \\
Qwen/Qwen2.5-0.5B & 0.9172 & 0.205 & 0.8877 & 0.8324 & 0.9309 & 0.6276 & 0.0861 & 0.3053 \\
Qwen/Qwen2.5-7B & 0.9402 & 0.4406 & 0.9297 & 0.865 & 0.947 & 0.6745 & 0.4981 & 0.3079 \\
Qwen/Qwen3-0.6B & 0.9242 & 0.3439 & 0.905 & 0.8483 & 0.9277 & 0.6551 & 0.1723 & 0.2528 \\
Qwen/Qwen3-1.7B & 0.9315 & 0.3899 & 0.9041 & 0.8659 & 0.9309 & 0.6662 & 0.191 & 0.3927 \\
Qwen/Qwen3-4B & 0.9409 & 0.4413 & 0.9224 & 0.8642 & 0.9382 & 0.6746 & 0.412 & 0.3582 \\
TUM/GottBERT_large & 0.9363 & 0.4187 & 0.9269 & 0.8114 & 0.9325 & 0.6602 & 0.2884 & 0.1541 \\
benjamin/gerpt2 & 0.913 & 0.237 & 0.9233 & 0.7787 & 0.9325 & 0.6215 & 0.0936 & 0.2814 \\
benjamin/gerpt2-large & 0.9354 & 0.4496 & 0.9352 & 0.8583 & 0.9414 & 0.6573 & 0.382 & 0.3214 \\
bert-base-german-cased & 0.9106 & 0.2998 & 0.9151 & 0.8516 & 0.9382 & 0.6349 & 0.2734 & 0.3088 \\
bigscience/bloomz-560m & 0.9142 & 0.2422 & 0.8831 & 0.7804 & 0.9293 & 0.5909 & 0.1086 & 0.2884 \\
bigscience/mt0-small & 0.2901 & 0.0001 & 0.1023 & 0.0411 & 0.0691 & 0.1417 & 0.0487 & 0.1084 \\
dbmdz/german-gpt2 & 0.9169 & 0.3177 & 0.916 & 0.7469 & 0.9341 & 0.6099 & 0.1348 & 0.1506 \\
deepset/gbert-base & 0.9238 & 0.3864 & 0.9288 & 0.8433 & 0.9333 & 0.6419 & 0.2921 & 0.2218 \\
deepset/gbert-large & 0.9367 & 0.567 & 0.9479 & 0.8776 & 0.943 & 0.6617 & 0.3783 & 0.307 \\
deepset/gelectra-base & 0.9213 & 0.2913 & 0.9059 & 0.798 & 0.9365 & 0.5949 & 0.0824 & 0.1475 \\
deepset/gelectra-large & 0.9365 & 0.5359 & 0.9279 & 0.8575 & 0.9398 & 0.646 & 0.191 & 0.2475 \\
facebook/mbart-large-50 & 0.9348 & 0.4377 & 0.9224 & 0.8265 & 0.9317 & 0.6223 & 0.0899 & 0.2625 \\
flair/bueble-lm-2b & 0.9348 & 0.5534 & 0.9315 & 0.8726 & 0.9438 & 0.6692 & 0.4382 & 0.3581 \\
gerturax/gerturax-1 & 0.9297 & 0.4332 & 0.9324 & 0.8365 & 0.9486 & 0.6319 & 0.176 & 0.2811 \\
gerturax/gerturax-2 & 0.9348 & 0.4435 & 0.937 & 0.855 & 0.943 & 0.6379 & 0.2322 & 0.2137 \\
gerturax/gerturax-3 & 0.9336 & 0.4276 & 0.937 & 0.8433 & 0.9462 & 0.6355 & 0.1835 & 0.222 \\
malteos/bloom-6b4-clp-german & 0.9479 & 0.4273 & 0.947 & 0.8835 & 0.9494 & 0.6778 & 0.5019 & 0.3913 \\
maxidl/DOSMo-7B-v0.2 & 0.9522 & 0.5766 & 0.9452 & 0.891 & 0.947 & 0.6789 & 0.5805 & 0.4012 \\
meta-llama/Llama-3.2-1B & 0.9343 & 0.3419 & 0.9096 & 0.8718 & 0.9382 & 0.6667 & 0.3708 & 0.3569 \\
meta-llama/Llama-3.2-3B & 0.9394 & 0.4551 & 0.9233 & 0.8667 & 0.9382 & 0.6762 & 0.4944 & 0.3904 \\
meta-llama/Meta-Llama-3.1-8B & 0.9477 & 0.51 & 0.9361 & 0.8734 & 0.9494 & 0.6773 & 0.5843 & 0.4537 \\
uklfr/gottbert-base & 0.9261 & 0.3868 & 0.9068 & 0.8558 & 0.9365 & 0.64 & 0.1573 & 0.1309 \\
utter-project/EuroLLM-1.7B & 0.9405 & 0.4965 & 0.9342 & 0.8961 & 0.9486 & 0.6669 & 0.5281 & 0.4104 \\
\bottomrule
\end{tabular}
\end{table}
